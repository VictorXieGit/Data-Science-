{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 - Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from importlib import reload\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from torchvision.transforms.functional import pil_to_tensor\n",
    "from torchsummary import summary\n",
    "import pathlib\n",
    "import sys\n",
    "\n",
    "data_path = pathlib.Path().cwd() / \"..\" / \"data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Python version -> {sys.version}\")  # 3.12.3\n",
    "print(f\"torch version -> {torch.__version__}\")  # 2.8.0+cu128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - MNIST Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = torchvision.datasets.MNIST(root=data_path, transform=pil_to_tensor, train=True)\n",
    "train, test = torch.utils.data.random_split(mnist, [0.7, 0.3])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train, batch_size=512, shuffle=True, num_workers=4\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test, batch_size=512, shuffle=True, num_workers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.empty(len(train), 28, 28)\n",
    "y_train = torch.empty(len(train), 1)\n",
    "for i, xytrain in enumerate(train):\n",
    "    xi, yi = xytrain\n",
    "    x_train[i] = xi.reshape(28, 28)\n",
    "    y_train[i] = yi\n",
    "\n",
    "\n",
    "x_test = torch.empty(len(test), 28, 28)\n",
    "y_test = torch.empty(len(test), 1)\n",
    "for i, xytest in enumerate(test):\n",
    "    xi, yi = xytest\n",
    "    x_test[i] = xi.reshape(28, 28)\n",
    "    y_test[i] = yi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(9):\n",
    "    plt.subplot(3, 3, i + 1)\n",
    "    j = torch.randint(low=0, high=42000, size=(1,))\n",
    "    digit = x_train[j]\n",
    "    label = y_train[j]\n",
    "    plt.imshow(digit.reshape(28, 28, 1))\n",
    "    plt.title(label)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 - Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Before normalization : Min={}, max={}\".format(x_train.min(), x_train.max()))\n",
    "\n",
    "xmax = x_train.max()\n",
    "x_train = x_train / xmax\n",
    "x_test = x_test / xmax\n",
    "\n",
    "print(\"After normalization  : Min={}, max={}\".format(x_train.min(), x_train.max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 - Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvMNIST(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvMNIST, self).__init__()\n",
    "        self.input_layers = torch.nn.Sequential(\n",
    "            torch.nn.LazyConv2d(8, (3, 3)),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d((2, 2)),\n",
    "            torch.nn.Dropout(0.2),\n",
    "        )\n",
    "        self.hidden_layer1 = torch.nn.Sequential(\n",
    "            torch.nn.LazyConv2d(16, (3, 3)),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d((2, 2)),\n",
    "            torch.nn.Dropout(0.2),\n",
    "        )\n",
    "        self.hidden_layer2 = torch.nn.Sequential(\n",
    "            torch.nn.Flatten(\n",
    "                1,\n",
    "            ),\n",
    "            torch.nn.LazyLinear(100),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.5),\n",
    "        )\n",
    "        self.out_layer = torch.nn.Sequential(\n",
    "            torch.nn.LazyLinear(10),\n",
    "            torch.nn.Softmax(-1),\n",
    "        )\n",
    "        self.all_layers = torch.nn.Sequential(\n",
    "            self.input_layers, self.hidden_layer1, self.hidden_layer2, self.out_layer\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.all_layers(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvMNIST()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import tqdm\n",
    "\n",
    "loss_function = CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(digit.reshape(1, 1, 28, 28) / 256.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5 - Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader):\n",
    "    ce_loss_tot = 0.0\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            digits, labels = data\n",
    "            digits = digits / 256.0\n",
    "            prediction = model(digits)\n",
    "            ce_loss_tot += loss_function(prediction, labels).item() / len(loader)\n",
    "    return ce_loss_tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvMNIST()\n",
    "optimizer = Adam(model.parameters(), lr=0.01, betas=(0.9, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_epoch = evaluate(model, train_loader)\n",
    "test_loss_epoch = evaluate(model, test_loader)\n",
    "print(train_loss_epoch, test_loss_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predictions for untrained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_softmax = model(x_test.reshape(18000, 1, 28, 28))\n",
    "y_predictions = torch.argmax(y_softmax, axis=-1)\n",
    "misclassified_indices = torch.where((y_predictions - y_test.squeeze()) != 0)\n",
    "print(\n",
    "    confusion_matrix(y_test, y_pred=y_predictions)\n",
    ")  # order matters! (actual, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 16\n",
    "model.train = True\n",
    "epochs_bar = tqdm.tqdm(range(epochs))\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "for epoch in epochs_bar:\n",
    "    train_loss_epoch = 0.0\n",
    "    for train_data in train_loader:\n",
    "        digits, labels = train_data\n",
    "        digits = digits / 256.0\n",
    "        prediction = model(digits)\n",
    "        cross_entropy_loss = loss_function(prediction, labels)\n",
    "        optimizer.zero_grad()\n",
    "        cross_entropy_loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss_epoch += cross_entropy_loss.detach().item() / len(train_loader)\n",
    "\n",
    "    train_loss.append(train_loss_epoch)\n",
    "    with torch.no_grad():\n",
    "        test_loss_epoch = evaluate(model, test_loader)\n",
    "        val_loss.append(test_loss_epoch)\n",
    "    epochs_bar.set_description(\n",
    "        f\"train loss: {train_loss_epoch:.4f}, val loss: {test_loss_epoch:.4f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_loss)\n",
    "plt.plot(val_loss)\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"CrossEntropyLoss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6 - Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = evaluate(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = [i for i in range(len(x_test)) if y_predictions[i] != y_test[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_softmax = model(x_test.reshape(18000, 1, 28, 28))\n",
    "y_predictions = torch.argmax(y_softmax, axis=-1)\n",
    "misclassified_indices = torch.where((y_predictions - y_test.squeeze()) != 0)\n",
    "print(\n",
    "    confusion_matrix(y_test, y_pred=y_predictions)\n",
    ")  # order matters! (actual, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 15))\n",
    "for i in range(25):\n",
    "    plt.subplot(5, 5, i + 1)\n",
    "    j = torch.randint(low=0, high=18000, size=(1,))\n",
    "    digit = x_test[j]\n",
    "    y_pred = model(digit.reshape(1, 1, 28, 28)).argmax()\n",
    "    label = y_test[j].item()\n",
    "    plt.imshow(digit.reshape(28, 28, 1))\n",
    "    if int(label) == y_pred:\n",
    "        plt.title(f\"True: {int(label)}, pred: {y_pred}, v\", color=\"g\")\n",
    "    else:\n",
    "        plt.title(f\"True: {int(label)}, pred: {y_pred}, x\", color=\"r\")\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 15))\n",
    "for i in range(25):\n",
    "    plt.subplot(5, 5, i + 1)\n",
    "    j = misclassified_indices[0][i]\n",
    "    digit = x_test[j]\n",
    "    # y_pred = model(digit.reshape(1, 1, 28, 28)).argmax()\n",
    "    label = y_test[j].item()\n",
    "    plt.imshow(digit.reshape(28, 28, 1))\n",
    "    plt.title(f\"True: {label}, pred: {y_predictions[j]}, {y_softmax[j].max():.4f}\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "visu-sandbox (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
