comparaison <- data.frame(
Modèle = c("AR(1) + désaisonnalisation", "ARIMA direct"),
MSE = c(mse_ar1, mse_arima),
MAPE = c(mape_ar1, mape_arima)
)
print(comparaison)
plot(mois, previsions_derniere, type = "l", col = "red", lwd = 2,
main = "Prévisions vs Réalité - Dernière année",
xlab = "Mois", ylab = "Température (°C)",
ylim = range(c(previsions_derniere, realite_derniere)))
lines(mois, realite_derniere, col = "blue", lwd = 2, type = "o")
points(mois, realite_derniere, col = "blue", pch = 16)
# Ajouter la zone d'incertitude en fond rouge transparent
polygon(c(mois, rev(mois)),
c(previsions_lower, rev(previsions_upper)),
col = rgb(1, 0, 0, 0.2), border = NA)
# Redessiner les lignes par-dessus
lines(mois, previsions_derniere, col = "red", lwd = 2)
lines(mois, realite_derniere, col = "blue", lwd = 2, type = "o")
points(mois, realite_derniere, col = "blue", pch = 16)
# Mettre à jour la légende
legend("bottomright",
legend = c("Prévisions", "IC 95%", "Réalité"),
col = c("red", rgb(1, 0, 0, 0.3), "blue"),
lwd = c(2, 10, 2), pch = c(NA, NA, 16),
bty = "n",
cex = 0.1 )
# 6. METRIQUES
mse <- mean((previsions_derniere - realite_derniere)^2)
mape <- mean(abs((previsions_derniere - realite_derniere)/realite_derniere)) * 100
rmse <- sqrt(mse)
cat("=== PERFORMANCE ===\n")
cat("MSE:", round(mse, 3), "\n")
cat("RMSE:", round(rmse, 3), "°C\n")
cat("MAPE:", round(mape, 2), "%\n")
# 7. TABLEAU détaillé
comparaison <- data.frame(
Mois = month.abb,
Indice = indices_derniere,
Réel = round(realite_derniere, 1),
Prévision = round(previsions_derniere, 1),
Erreur = round(previsions_derniere - realite_derniere, 1)
)
print("=== COMPARAISON DÉTAILLÉE ===")
print(comparaison)
#Avec arima auto
# Extraire les prévisions auto.arima pour la dernière année
previsions_auto_derniere <- previsions_auto$mean
# Tracer comparaison sur les 12 derniers mois
plot(mois, as.numeric(previsions_auto_derniere), type = "l", col = "red", lwd = 2,
main = "ARIMA auto vs Réalité - Dernière année",
xlab = "Mois", ylab = "Température (°C)",
ylim = range(c(previsions_auto_derniere, realite_derniere)))
# Ajouter IC 95%
polygon(c(mois, rev(mois)),
c(previsions_auto$lower[,2], rev(previsions_auto$upper[,2])),
col = rgb(1, 0, 0, 0.2), border = NA)
# Relancer les lignes
lines(mois, as.numeric(previsions_auto_derniere), col = "red", lwd = 2)
lines(mois, realite_derniere, col = "blue", lwd = 2, type = "o")
# Légende
legend("topleft",
legend = c("ARIMA auto", "IC 95%", "Réalité"),
col = c("red", rgb(1, 0, 0, 0.3), "blue"),
lwd = c(2, 10, 2), pch = c(NA, NA, 1),
bty = "n",
cex = 0.1 )
# Comparaison avec tes prévisions AR(1) + désaisonnalisation manuelle
mse_ar1 <- mean((previsions_derniere - realite_derniere)^2)
mse_arima <- mean((as.numeric(previsions_arima$mean) - realite_derniere)^2)
mape_ar1 <- mean(abs((previsions_derniere - realite_derniere)/realite_derniere)) * 100
mape_arima <- mean(abs((as.numeric(previsions_arima$mean) - realite_derniere)/realite_derniere)) * 100
# Tableau comparatif
comparaison <- data.frame(
Modèle = c("AR(1) + désaisonnalisation", "ARIMA direct"),
MSE = c(mse_ar1, mse_arima),
MAPE = c(mape_ar1, mape_arima)
)
plot(mois, previsions_derniere, type = "l", col = "red", lwd = 2,
main = "Prévisions vs Réalité - Dernière année",
xlab = "Mois", ylab = "Température (°C)",
ylim = range(c(previsions_derniere, realite_derniere)))
lines(mois, realite_derniere, col = "blue", lwd = 2, type = "o")
points(mois, realite_derniere, col = "blue", pch = 16)
# Ajouter la zone d'incertitude en fond rouge transparent
polygon(c(mois, rev(mois)),
c(previsions_lower, rev(previsions_upper)),
col = rgb(1, 0, 0, 0.2), border = NA)
# Redessiner les lignes par-dessus
lines(mois, previsions_derniere, col = "red", lwd = 2)
lines(mois, realite_derniere, col = "blue", lwd = 2, type = "o")
points(mois, realite_derniere, col = "blue", pch = 16)
# Mettre à jour la légende
legend("bottomright",
legend = c("Prévisions", "IC 95%", "Réalité"),
col = c("red", rgb(1, 0, 0, 0.3), "blue"),
lwd = c(2, 10, 2), pch = c(NA, NA, 16),
bty = "n",
cex = 0.3 )
# 6. METRIQUES
mse <- mean((previsions_derniere - realite_derniere)^2)
mape <- mean(abs((previsions_derniere - realite_derniere)/realite_derniere)) * 100
rmse <- sqrt(mse)
cat("=== PERFORMANCE ===\n")
cat("MSE:", round(mse, 3), "\n")
cat("RMSE:", round(rmse, 3), "°C\n")
cat("MAPE:", round(mape, 2), "%\n")
# 7. TABLEAU détaillé
comparaison <- data.frame(
Mois = month.abb,
Indice = indices_derniere,
Réel = round(realite_derniere, 1),
Prévision = round(previsions_derniere, 1),
Erreur = round(previsions_derniere - realite_derniere, 1)
)
print("=== COMPARAISON DÉTAILLÉE ===")
print(comparaison)
#Avec arima auto
# Extraire les prévisions auto.arima pour la dernière année
previsions_auto_derniere <- previsions_auto$mean
# Tracer comparaison sur les 12 derniers mois
plot(mois, as.numeric(previsions_auto_derniere), type = "l", col = "red", lwd = 2,
main = "ARIMA auto vs Réalité - Dernière année",
xlab = "Mois", ylab = "Température (°C)",
ylim = range(c(previsions_auto_derniere, realite_derniere)))
# Ajouter IC 95%
polygon(c(mois, rev(mois)),
c(previsions_auto$lower[,2], rev(previsions_auto$upper[,2])),
col = rgb(1, 0, 0, 0.2), border = NA)
# Relancer les lignes
lines(mois, as.numeric(previsions_auto_derniere), col = "red", lwd = 2)
lines(mois, realite_derniere, col = "blue", lwd = 2, type = "o")
# Légende
legend("bottomright",
legend = c("ARIMA auto", "IC 95%", "Réalité"),
col = c("red", rgb(1, 0, 0, 0.3), "blue"),
lwd = c(2, 10, 2), pch = c(NA, NA, 1),
bty = "n",
cex = 0.3 )
# Comparaison avec tes prévisions AR(1) + désaisonnalisation manuelle
mse_ar1 <- mean((previsions_derniere - realite_derniere)^2)
mse_arima <- mean((as.numeric(previsions_arima$mean) - realite_derniere)^2)
mape_ar1 <- mean(abs((previsions_derniere - realite_derniere)/realite_derniere)) * 100
mape_arima <- mean(abs((as.numeric(previsions_arima$mean) - realite_derniere)/realite_derniere)) * 100
# Tableau comparatif
comparaison <- data.frame(
Modèle = c("AR(1) + désaisonnalisation", "ARIMA direct"),
MSE = c(mse_ar1, mse_arima),
MAPE = c(mape_ar1, mape_arima)
)
print(comparaison)
plot(mois, realite_derniere, type = "o", col = "black", lwd = 2,
main = "Comparaison AR1 vs ARIMA auto",
xlab = "Mois", ylab = "Température (°C)")
lines(mois, previsions_derniere, col = "red", lwd = 2)  # Ton AR1
lines(mois, as.numeric(previsions_auto_derniere), col = "blue", lwd = 2)  # ARIMA auto
legend("topright", bty = "n", cex = 0.7,
legend = c("Réalité", "AR1", "ARIMA auto"),
col = c("black", "red", "blue"), lwd = 2)
plot(mois, realite_derniere, type = "o", col = "black", lwd = 2,
main = "Comparaison AR1 vs ARIMA auto",
xlab = "Mois", ylab = "Température (°C)")
lines(mois, previsions_derniere, col = "red", lwd = 2)  # Ton AR1
lines(mois, as.numeric(previsions_auto_derniere), col = "blue", lwd = 2)  # ARIMA auto
legend("topright", bty = "n", cex = 0.7,
legend = c("Réalité", "AR1", "ARIMA auto"),
col = c("black", "red", "blue"), lwd = 2)
par(mfrow=c(1,1))
plot(mois, realite_derniere, type = "o", col = "black", lwd = 2,
main = "Comparaison AR1 vs ARIMA auto",
xlab = "Mois", ylab = "Température (°C)")
lines(mois, previsions_derniere, col = "red", lwd = 2)  # Ton AR1
lines(mois, as.numeric(previsions_auto_derniere), col = "blue", lwd = 2)  # ARIMA auto
legend("topright", bty = "n", cex = 0.7,
legend = c("Réalité", "AR1", "ARIMA auto"),
col = c("black", "red", "blue"), lwd = 2)
plot(mois, realite_derniere, type = "o", col = "black", lwd = 2,
main = "Comparaison AR1 vs ARIMA auto",
xlab = "Mois", ylab = "Température (°C)")
lines(mois, previsions_derniere, col = "red", lwd = 2)  # Ton AR1
lines(mois, as.numeric(previsions_auto_derniere), col = "blue", lwd = 2)  # ARIMA auto
legend("topright", bty = "n", cex = 0.7,
legend = c("Réalité", "AR1", "ARIMA auto"),
col = c("black", "red", "blue"), lwd = 2)
### exemple pour des donn?es al?atoire
nn = 50
library(stats)
library(graphics)
library(cluster)
library(fpc)
### exemple pour des donn?es al?atoire
nn = 50
x <- rbind(matrix(rnorm(2*nn, mean = 0.5, sd = 0.3), ncol = 2),
matrix(rnorm(2*nn, mean = 4, sd = 0.3), ncol = 2),
matrix(rnorm(2*nn, mean = 2, sd = 0.5), ncol = 2))
colnames(x) <- c("x", "y")
plot(x, col = cbind(matrix(1,50,1), matrix(2,50,1), matrix(3,50,1)))
title('Distribution initiale')
n <- dim(x)[1]
cl3 <- kmeans(x,3)
plot(x, col=cl3$cluster)
title("Apres le k-means")
points(cl3$centers, col = 'yellow', pch = 8, cex=2)
distance<- function(x1,x2){
dist <- 0
for (i in 1:length(x1)){
dist <- dist + ((x1[i]-x2[i]**2))
}
return sqrt(dist)
distance<- function(x1,x2){
dist <- 0
for (i in 1:length(x1)){
dist <- dist + ((x1[i]-x2[i]**2))
}
return dist**0.5
print(ownkmean(x,3))
distance <- function(a, b) sqrt(sum((a - b)^2))
### exemple pour des donn?es al?atoire
nn = 50
x <- rbind(matrix(rnorm(2*nn, mean = 0.5, sd = 0.3), ncol = 2),
matrix(rnorm(2*nn, mean = 4, sd = 0.3), ncol = 2),
matrix(rnorm(2*nn, mean = 2, sd = 0.5), ncol = 2))
colnames(x) <- c("x", "y")
plot(x, col = cbind(matrix(1,50,1), matrix(2,50,1), matrix(3,50,1)))
title('Distribution initiale')
n <- dim(x)[1]
distance <- function(a, b) sqrt(sum((a - b)^2))
ownkmean <- function(data, k,iteration){
n <- nrow(data)
Ks <- data[sample(nrow(data),k),]
affect <- integer(n)
for (i in 1:iteration){
for (p in 1:n){
distances <- sapply(Ks,1, function(x)  distance(x, data[p]))
idx <- which.min(distances)
affect[p] <- idx
}
for(l in 1:k){
Ks[l, ] <- colMeans(data[affect == l, , drop = FALSE])    }
}
return (affect)
}
print(ownkmean(x,3))
print(ownkmean(x,3,10))
ownkmean <- function(data, k,iteration){
n <- nrow(data)
Ks <- data[sample(nrow(data),k),]
affect <- integer(n)
for (i in 1:iteration){
for (p in 1:n){
distances <- sapply(Ks,1, function(x)  distance(x, data[p,]))
idx <- which.min(distances)
affect[p] <- idx
}
for(l in 1:k){
Ks[l, ] <- colMeans(data[affect == l, , drop = FALSE])    }
}
return (affect)
}
print(ownkmean(x,3,10))
ownkmean <- function(data, k,iteration){
n <- nrow(data)
Ks <- data[sample(nrow(data),k),]
affect <- integer(n)
for (i in 1:iteration){
for (p in 1:n){
distances <- apply(Ks,1, function(x)  distance(x, data[p,]))
idx <- which.min(distances)
affect[p] <- idx
}
for(l in 1:k){
Ks[l, ] <- colMeans(data[affect == l, , drop = FALSE])    }
}
return (affect)
}
print(ownkmean(x,3,10))
affectation <- ownkmean(x,3,10)
for (i in 1:max(affectation)){
plot(x[affectation == i])
}
colors <- rainbow(max(affectation))
plot(data[,1], data[,2], type = "n", xlab = "X1", ylab = "X2")  # canvas vide
for(i in 1:max(affectation)) {
points(data[affectation == i, 1], data[affectation == i, 2], col = colors[i], pch = 19)
}
data[,1]
plot(x[,1], x[,2], type = "n", xlab = "X1", ylab = "X2")  # canvas vide
for(i in 1:max(affectation)) {
points(x[affectation == i, 1], x[affectation == i, 2], col = colors[i], pch = 19)
}
data <- read.table("md_classes(3).csv")
data <- read.table("md_classes (3).csv")
data <- read.csv(file = "md_classes (3).csv")
data <- read.csv(file = "C:\\Users\\victo\\OneDrive\\Bureau\\COURS général\\cours mine 2526\\sdd\\UP3 Machine Learning\\Clustering et classif knn\\Données pour le TP-20251118\\md_classes (3).csv")
setwd("C:\\Users\\victo\\OneDrive\\Bureau\\COURS général\\cours mine 2526\\sdd\\UP3 Machine Learning\\Clustering et classif knn\\Données pour le TP-20251118")
data <- read.csv(file = "md_classes (3).csv")
data <- read.csv(file = "md_classes (3).csv", header = TRUE)
data[,1]
View(data)
View(data)
K = 2:10;
J<- matrix(0,length(K),1);
JJ<- matrix(0,length(K),1);
for (k in K)
{
cl <- kmeans(x,k)
plot(x, col=cl$cluster)
title(paste("Apres le k-means pour k =", k))
#points(cl$centers, col = 'yellow', pch = 8, cex=2)
J[k-1] <- 1/n * cl$tot.withinss
xx <- x- cl$center[cl$cluster]
JJ[k-1] <- 1/n * sum(xx * xx)
}
plot(K,JJ, type='l')
points(K,J)
# max K à tester
Kmax <- 10
# vecteur pour stocker les inerties
inerties <- numeric(Kmax)
for (k in 1:Kmax) {
km <- kmeans(data, centers = k, nstart = 10)
# inertie = somme des distances au carré (tot.withinss)
inerties[k] <- km$tot.withinss
}
data <- data[,-1]
data
data <- read.csv(file = "md_classes (3).csv", header = TRUE)
data <- data[,-1]
data
data <- read.csv(file = "md_classes (3).csv", header = TRUE)
data
data[0]
data[1]
data[1,]
data <- read.csv(file = "md_classes (3).csv", header = TRUE,sep =";" )
data
data <- data[,-1]
data
# max K à tester
Kmax <- 10
# vecteur pour stocker les inerties
inerties <- numeric(Kmax)
for (k in 1:Kmax) {
km <- kmeans(data, centers = k, nstart = 10)
# inertie = somme des distances au carré (tot.withinss)
inerties[k] <- km$tot.withinss
}
# tracer la courbe
plot(1:Kmax, inerties, type = "b", pch = 19,
xlab = "Nombre de clusters K",
ylab = "Inertie intra-cluster",
main = "Méthode du coude")
affectdata <- ownkmean(data,15,100)
affectdata <- ownkmean(data,15,10)
affectdata
data <- data[,-c(1,ncol(data))]
data
# max K à tester
Kmax <- 10
# vecteur pour stocker les inerties
inerties <- numeric(Kmax)
for (k in 1:Kmax) {
km <- kmeans(data, centers = k, nstart = 10)
# inertie = somme des distances au carré (tot.withinss)
inerties[k] <- km$tot.withinss
}
# tracer la courbe
plot(1:Kmax, inerties, type = "b", pch = 19,
xlab = "Nombre de clusters K",
ylab = "Inertie intra-cluster",
main = "Méthode du coude")
affectdata <- ownkmean(data,15,10)
colors <- rainbow(max(affectdata))
plot(data[,1], data[,2], type = "n", xlab = "X1", ylab = "X2")  # canvas vide
for(i in 1:max(affectation)) {
points(data[affectdata == i, 1], data[affecdata == i, 2], col = colors[i], pch = 19)
}
for(i in 1:max(affectation)) {
points(data[affectdata == i, 1], data[affectdata == i, 2], col = colors[i], pch = 19)
}
data[,1]
affecdata
affectdata
plot(data[,1], data[,2], type = "n", xlab = "X1", ylab = "X2")  # canvas vide
for(i in 1:max(affectation)) {
points(data[affectdata == i, 1], data[affectdata == i, 2], col = colors[i], pch = 19)
}
data[,0]
data
data <- read.csv(file = "md_classes (3).csv", header = TRUE,sep =";" )
data <- data[,-c(1,ncol(data))]
data
affectation <- ownkmean(x,3,10)
affectdata <- ownkmean(data,15,10)
colors <- rainbow(max(affectdata))
plot(data[,1], data[,2], type = "n", xlab = "X1", ylab = "X2")  # canvas vide
for(i in 1:max(affectation)) {
points(data[affectdata == i, 1], data[affectdata == i, 2], col = colors[i], pch = 19)
}
max(affectdata)
data[affectdata == 1, 1]
data[affectdata == 1, 2]
meth <- kmeans(data,15)
View(meth)
nn = 50
x <- rbind(matrix(rnorm(2*nn, mean = 0.5, sd = 0.3), ncol = 2),
matrix(rnorm(2*nn, mean = 4, sd = 0.3), ncol = 2),
matrix(rnorm(2*nn, mean = 2, sd = 0.5), ncol = 2))
colnames(x) <- c("x", "y")
plot(x, col = cbind(matrix(1,50,1), matrix(2,50,1), matrix(3,50,1)))
title('Distribution initiale')
n <- dim(x)[1]
cl3 <- kmeans(x,3)
plot(x, col=cl3$cluster)
title("Apres le k-means")
points(cl3$centers, col = 'yellow', pch = 8, cex=2)
plot(x, col=cl3$cluster)
title("Apres le k-means")
points(cl3$centers, col = 'yellow', pch = 8, cex=2)
cl3
# Charger les données
data(iris)
head(iris)
# On prend uniquement les variables numériques
X <- iris[, 1:4]
# Les classes réelles pour comparaison
true_labels <- iris$Species
# Les classes réelles pour comparaison
true_labels <- iris$Species
set.seed(123)
k <- 3
km <- kmeans(X, centers = k, nstart = 25)
# cluster assignés
km$cluster
# Comparaison avec les vraies classes
table(km$cluster, true_labels)
# Visualisation (2 premières dimensions)
plot(X[,1], X[,2], col = km$cluster, pch=19,
xlab="Sepal.Length", ylab="Sepal.Width",
main="K-means clustering")
points(km$centers[,1:2], col=1:3, pch=8, cex=2)
# Calcul de la distance
dist_X <- dist(X)
# Clustering hiérarchique
hc <- hclust(dist_X, method="ward.D2")
# Dendrogramme
plot(hc, labels = false, main="Dendrogramme CHA")
# Dendrogramme
plot(hc, labels = FALSE, main="Dendrogramme CHA")
# Découper en 3 clusters
hc_clusters <- cutree(hc, k = 3)
# Comparaison
table(hc_clusters, true_labels)
# Visualisation
plot(X[,1], X[,2], col = hc_clusters, pch=19,
xlab="Sepal.Length", ylab="Sepal.Width",
main="Clustering hiérarchique")
install.packages("dbscan")   # si pas déjà installé
library(dbscan)
# epsilon choisi avec kNNdistplot éventuellement
db <- dbscan(X, eps = 0.6, minPts = 5)
# Clusters assignés (0 = bruit)
db$cluster
# Comparaison
table(db$cluster, true_labels)
# Visualisation
plot(X[,1], X[,2], col = db$cluster + 1, pch=19,
xlab="Sepal.Length", ylab="Sepal.Width",
main="DBSCAN clustering")
# epsilon choisi avec kNNdistplot éventuellement
db <- dbscan(X, eps = 0.4, minPts = 5)
# Clusters assignés (0 = bruit)
db$cluster
# Comparaison
table(db$cluster, true_labels)
# Visualisation
plot(X[,1], X[,2], col = db$cluster + 1, pch=19,
xlab="Sepal.Length", ylab="Sepal.Width",
main="DBSCAN clustering")
# epsilon choisi avec kNNdistplot éventuellement
db <- dbscan(X, eps = 0.4, minPts = 3)
# Clusters assignés (0 = bruit)
db$cluster
# Comparaison
table(db$cluster, true_labels)
# Visualisation
plot(X[,1], X[,2], col = db$cluster + 1, pch=19,
xlab="Sepal.Length", ylab="Sepal.Width",
main="DBSCAN clustering")
# epsilon choisi avec kNNdistplot éventuellement
db <- dbscan(X, eps = 0.5, minPts = 3)
# Clusters assignés (0 = bruit)
db$cluster
# Comparaison
table(db$cluster, true_labels)
# Visualisation
plot(X[,1], X[,2], col = db$cluster + 1, pch=19,
xlab="Sepal.Length", ylab="Sepal.Width",
main="DBSCAN clustering")
library(class)
# Diviser en train/test (70/30)
set.seed(123)
train_idx <- sample(1:nrow(X), 0.7*nrow(X))
X_train <- X[train_idx, ]
y_train <- true_labels[train_idx]
X_test <- X[-train_idx, ]
y_test <- true_labels[-train_idx]
# Appliquer K-NN
k_nn <- 5
y_pred <- knn(X_train, X_test, y_train, k = k_nn)
# Matrice de confusion
table(y_test, y_pred)
