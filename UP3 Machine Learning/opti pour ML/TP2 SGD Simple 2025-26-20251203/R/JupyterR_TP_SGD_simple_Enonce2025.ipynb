{
 "cells": [
  {
   "cell_type": "raw",
   "id": "6ee81cd0",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"TP2 on stochastic gradient optimization\"\n",
    "subtitle: \"UP3, Optimization for Machine Learning\"\n",
    "author: \n",
    "  - Rodolphe Le Riche^[CNRS LIMOS, Mines St-Etienne, UCA]\n",
    "  - Didier Rullière^[Mines Saint-Etienne, CNRS LIMOS]\n",
    "date: \"Séance 2, cours du mercredi 3 décembre 2025 matin\"\n",
    "output:\n",
    "  html_document: default\n",
    "  pdf_document: default\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb019e6-04a2-4206-9779-a197835564e6",
   "metadata": {},
   "source": [
    "La feuille de style CSS spécifie des options de style du notebook, vous pouvez ignorer cette partie liée à l affichage du notebook. sur Jupyter, lancer Run > Run all cells avant File > Save and export as > HTML."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446b5fc5",
   "metadata": {},
   "source": [
    "On commence par fixer une graine au générateur aléatoire, pour rendre les résultats reproductibles. Les séquences de nombres aléatoires générées seront toujours les mêmes (sauf si on change la graine bien sûr!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd30bb9",
   "metadata": {
    "name": "seed"
   },
   "outputs": [],
   "source": [
    "set.seed(123456)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f19ea2f",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "On rappelle le principe de l'algorithme de Robbins-Monro:\n",
    "$$ \\Theta_{n+1} = \\Theta_n - a_n G( \\Theta_n). $$\n",
    "à chaque étape, on descend dans la direction opposée du gradient, même lorsque ce dernier est entaché de bruit.\n",
    "\n",
    "# Exercice 0. Robbins-Monro minimal - code fourni\n",
    "\n",
    "Considérons la fonction $\\theta \\mapsto \\theta^2$, dont le minimum est en $\\theta=0$. La fonction n'est pas connue de l'expérimentateur, mais supposons que l'on puisse évaluer un gradient bruité de cette fonction,  $G: \\theta \\mapsto 2\\theta + \\epsilon$, où $\\epsilon$ est un bruit Gaussien centré et réduit.\n",
    "\n",
    "On peut programmer Robbins-Monro avec un pas $a$ constant:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8238b25a",
   "metadata": {
    "name": "question 0a"
   },
   "outputs": [],
   "source": [
    "G <- function(theta) {2*theta + rnorm(1, mean=0, sd=1)}\n",
    "a <- 0.5\n",
    "Theta <- vector(length=100)\n",
    "Theta[1] <- 4\n",
    "for(n in 1:99) {\n",
    "  Theta[n+1] <- Theta[n] - a * G(Theta[n])\n",
    "}\n",
    "plot(Theta, type=\"l\", col=\"blue\")\n",
    "abline(h=0, col=\"red\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6950fb",
   "metadata": {},
   "source": [
    "Que constatez-vous? comment l'interprétez-vous? sauriez-vous donner un argument mathématique à votre analyse?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c086438c",
   "metadata": {
    "name": "question 0a your answer"
   },
   "outputs": [],
   "source": [
    "# votre réponse en commentaire ici\n",
    "# votre réponse en commentaire ici\n",
    "# votre réponse en commentaire ici"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebc6165",
   "metadata": {},
   "source": [
    "A présent, considérons **le même code**, en remplaçant $a$ par $a/n$, de sorte que les pas sont décroissants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140a5d7b",
   "metadata": {
    "name": "question 0b"
   },
   "outputs": [],
   "source": [
    "G <- function(theta) {2*theta + rnorm(1, mean=0, sd=1)}\n",
    "a <- 0.5\n",
    "Theta <- vector(length=100)\n",
    "Theta[1] <- 4\n",
    "for(n in 1:99) {\n",
    "  Theta[n+1] <- Theta[n] - a/n * G(Theta[n])\n",
    "}\n",
    "plot(Theta, type=\"l\", col=\"blue\")\n",
    "abline(h=0, col=\"red\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf755d1",
   "metadata": {},
   "source": [
    "Que constatez-vous? comment l'interprétez-vous? sauriez-vous donner un argument mathématique à votre analyse?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f39c857",
   "metadata": {
    "name": "question 0b your answer"
   },
   "outputs": [],
   "source": [
    "# votre réponse en commentaire ici\n",
    "# votre réponse en commentaire ici\n",
    "# votre réponse en commentaire ici"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821e0782",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Vous pouvez faire quelques essais ici, modifier la décroissance, la valeur des pas, etc. L'exercice suivant revient sur ces aspects en détail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3fb6ac",
   "metadata": {
    "name": "exercice 0c"
   },
   "outputs": [],
   "source": [
    "# Votre code ici."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9752c32",
   "metadata": {},
   "source": [
    "# Exercice 1. Robbins-Monro sur fonction simple\n",
    "\n",
    "Nous allons illustrer la descente de gradient stochastique au moyen d'une fonction très simple. L'intérêt sera de pouvoir comprendre le fonctionnement des algorithmes et de pouvoir comparer empiriquement les vitesses de convergence. L'illustration se veut minimale, mais couvre néanmoins les principaux écueils possible de la convergence: *noise ball* et *biais*.\n",
    "\n",
    "On supposera ici que la fonction à minimiser est\n",
    "$$ f(\\theta) =\\frac{2\\theta^2-4\\theta+2}{\\theta^2+1}\\, .$$\n",
    "Bien sûr, on suppose qu'on ignore la vraie expression de $f$, et que l'experimentateur n'a accès qu'à des observations bruitées de la dérivée de $f$:\n",
    "$$ G(\\theta) = 4\\frac{\\theta^2-1}{(\\theta^2+1)^2} + \\epsilon(\\theta) \\, .$$\n",
    "où les $\\epsilon(\\theta)$ sont variables aléatoires gaussiennes centrées et réduite, mutuellement indépendantes.\n",
    "\n",
    "### Remarque:\n",
    "\n",
    "Cette situation où l'on observe seulement un gradient bruité est très commune en Machine Learning, notamment lors de l'optimisation de réseaux de neurones:\n",
    "\n",
    "* D'une part, le gradient est connu car la fonction appliquée est connue (une composition de fonctions d'activation).\n",
    "* D'autre part, le gradient est entaché de bruit, car évalué sur un échantillon alétoire de données.\n",
    "\n",
    "Vous pouvez visualiser la fonction à l'aide de graphiques R, mais aussi à l'aide d'outils en ligne, comme <https://www.desmos.com/calculator/9wjwoq6gxi> .\n",
    "\n",
    "## Question 1a. Gradient bruité et Robbins-Monro\n",
    "\n",
    "Programmer la fonction $G$ et l'algorithme de Robbins-Monro:\n",
    "$$ \\Theta_{n+1} = \\Theta_n - a_n G( \\Theta_n). $$\n",
    "On prendra des pas de la forme $$a_n=\\frac{a}{n^\\alpha}$$\n",
    "où $a$, $\\alpha$ sont des paramètres de la fonction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994aa7a7",
   "metadata": {
    "name": "question1a"
   },
   "outputs": [],
   "source": [
    "# votre code ici"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39e08df",
   "metadata": {},
   "source": [
    "## Question 1b. Pas décroissant doucement et *convergence*\n",
    "\n",
    "Tracer l'évolution de la suite $(\\Theta_n)_{n=1,2, ..., n_{max}}$ pour $\\alpha=1$ et pour différentes valeurs de $a$. On prendra par exemple une suite initialisée en $\\Theta_1 = 2$, on pourra utiliser, par exemple $n_{max}=20$. Dans un premier temps, on peut essayer avec une décroissance des pas donnée par $\\alpha=1$.\n",
    "\n",
    "Des amplitudes de pas $a$ vous semblent-elles meilleures que d'autres?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6489d0a3",
   "metadata": {
    "name": "question1b"
   },
   "outputs": [],
   "source": [
    "# votre code ici"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0673c3",
   "metadata": {},
   "source": [
    "## Question 1c. Décroissance trop lente. Pas constants et *noise ball*\n",
    "Montrer que lorsque $\\alpha=0$, c'est-à-dire lorsque les pas $a_n=a$ sont constants, la suite oscille dans une ``noise ball'' (cf. cours), pourquoi?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb733f0",
   "metadata": {
    "name": "question1c"
   },
   "outputs": [],
   "source": [
    "# votre code ici"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a090ad",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "## Question 1d. Pas décroissant trop vite et *biais*\n",
    "\n",
    "Montrer qu'à l'inverse, si la décroissance des pas est trop rapide, par exemple donnée par $\\alpha=2$, alors la suite semble souvent biaisée. Pourquoi?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64734bf8",
   "metadata": {
    "name": "question1d"
   },
   "outputs": [],
   "source": [
    "# votre code ici"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25abe61d",
   "metadata": {},
   "source": [
    "## Question 1e. quelques extensions libres\n",
    "\n",
    "* Vous pouvez vous amuser avec différentes valeurs de $a$ et de $\\alpha$.\n",
    "* Vous pouvez faire du l'averaging d'abscisse (postprocessing), constater que cela permet de diminuer $\\alpha$\n",
    "* Essayer de tracer les erreurs biais, variance et erreur totale en fonction du pas\n",
    "\n",
    "N'hésitez pas à traiter en priorité l'exercice suivant, avant de revenir aux extensions libres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f0fcff",
   "metadata": {
    "name": "question1e"
   },
   "outputs": [],
   "source": [
    "# votre code ici"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb7ec0b",
   "metadata": {},
   "source": [
    "## Exercice 2. Estimation du gradient\n",
    "\n",
    "On imagine désormais qu'un code de calcul renvoit, en fonction d'un paramètre $\\theta$, une valeur réelle (par exemple lorsqu'il simule un phénomène). Pour fixer les idées, disons qu'il s'agit d'un frottement $f(\\theta)$ à minimiser, en fonction d'un paramètre de forme $\\theta$. Le code de calcul s'exécute très lentement, donc on souhaite faire le moins d'évaluation possible pour optimiser le paramètre $\\theta$.\n",
    "\n",
    "Pour les besoins du TP, nous prendrons la fonction \n",
    "$$f(\\theta) := \\frac{2\\theta^2-4\\theta+2}{\\theta^2+1}$$\n",
    "\n",
    "## Question 2a. Descente en l'absence de bruit\n",
    "\n",
    "En l'absence de bruit, opérer une descente de gradient avec un budget de 50 évaluations de la fonction, en estimant le gradient par différence finie (ce qui requiert 2 évaluations par itération). Quel nouveau paramètre faut-il définir?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645ee159",
   "metadata": {
    "name": "question 2a"
   },
   "outputs": [],
   "source": [
    "# vos réponses ici"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a44c7d",
   "metadata": {},
   "source": [
    "## Question 2b. Gradient estimé et descente en présence de bruit\n",
    "\n",
    "Nous allons voir que la descente est beaucoup plus délicate en présence de bruit...\n",
    "\n",
    "Supposons que le code de calcul n'est pas parfait, de sorte que chaque évaluation de $f(\\theta)$ est entachée d'un bruit $\\epsilon$, bruit Gaussien centré (chaque évaluation de la fonction conduit à un nouveau tirage de $\\epsilon$), d'écart-type $0.1$.\n",
    "Par exemple, le code de calcul peut utiliser un maillage imprécis, des approximations numériques, ou dépendre de simulations physiques incertaines (essais en soufflerie par exemple).\n",
    "\n",
    "Adapter vos calculs pour faire la descente en présence du bruit epsilon.\n",
    "\n",
    "Quelles question se posent? La convergence vous semble-t-elle possible à obtenir? est-elle beaucoup plus lente en présence de bruit?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32805acc",
   "metadata": {
    "lines_to_next_cell": 0,
    "name": "question 2b"
   },
   "outputs": [],
   "source": [
    "# vos réponses ici"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3d1839",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "language,name,tags,-all",
   "main_language": "R",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.3.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
